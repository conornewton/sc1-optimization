---
    title: Statistical Computing - Portfolio 2
    author: Conor Newton
    output:
        pdf_document:
            toc: true
            toc_depth: 3
            number_sections: true
    fontsize: 12pt
---

\newpage

# Performance & Bugs

## Basic Error Handling in R

If we use error handling effectively in our code we can prevent many bugs arising in the first place. If bugs do occur, they can be easily identified by checking if an error has been thrown or a warning has been presented.

Error handling in R typically involves sending "signals". There are three ways we can send signals in our code. Here we list them in decreasing severity with the context they should be use in.

* **Errors**: We send an error when execution must stop unexpectedly. We call this **throwing** an error. The most elementary way of doing this is via the `stop()` function.
* **Warnings**: We can send an error when something wrong has occurred, however execution can continue. We can send warnings using the `warning()` function.
* **Messages**: A message can be sent to the user to inform them that an action has been completed. We can send messages using the `message()` function.

The `message()` function differs from `print()` since it writes its output to `stderr`. Warnings are also write their output to the `stderr` file. This makes it easy to identify the users output from `print` which is written to `stdout`.

It is particularly important to use these three functions correctly when developing a package. If we want to hide warnings and messages we can make use of the `suppressWarnings()` and the `suppressMessages()` functions.


A common function that should throw an error is division. An error should be thrown when division by $0$ is attempted. This can be done with `stop()`
```{r error=TRUE}
    div = function(a, b) {
        if (b == 0) stop("cannot divide by 0")
        else return(a / b)
    }
    div(4,0)
```

We can use `tryCatch` to catch handle the error that is thrown by `div` and continue execution.
```{r error=TRUE}
    #We can catch a possible error 
    tryCatch(
        #If div(4,0) throws an error we continue execution from this function
        error = function(cnd) {
            print("Please don't divide by 0 in the future")
        },
        div(4,0)
    )
```

## Profiling

Profiling is measuring the performance of a program, this includes both memory usage and the execution time. The most basic profiling tool in R is the `system.time()` function. This works by measuring the execution time of a given statement.

The library `rbenchmark` allows us the measure the execution speed of a block of code using a function `benchmark` that builds on top of `system.time()` with added convenience. We can easily the execution speed of different functions against each other and repeat the executions to get an average execution time. 

Below, we compare two methods for multiplying matrices. We can specify the functions we want to profile as the first arguments of `benchmark`. We have chosen to test each function 10 times using the replications parameter.
```{r error=TRUE}
library(rbenchmark)

n = 100
m = 100
A = matrix(runif(n*m), n, m)
B = matrix(runif(n*m), n, m)
C = matrix(runif(n*1),n, 1)

benchmark(
  "right" = { A %*% B %*% C},
  "left" = { A %*% (B %*% C) },
  replications = 10, order=NULL,
  columns = c("test", "replications", "elapsed",
              "user.self", "sys.self"))
```
We clearly see the second method is much faster!

The library `profmem` allows us to check how much memory a block of code is using and how it is allocated.
```{r}
library(profmem)
options(profmem.threshold = 2000) # Hides memory allocations less than 2000 bytes.
profmem({
    n = 100
    A = matrix(runif(n*n), n, n)
})
```

Of course there are many other libraries that do the same thing, these are just small and convenient libraries that I happened to stumble across and liked.

\newpage

# Matrices

There are typically two main implementations of matrices in programming languages: *dense* and *sparse*. Dense matrices are the standard matrices we have seen before. They are just a two dimensional data structure build on top of a `vector`. Sparse matrices are a special case; a large number of the entries of the matrix are 0's (or some other default value). These matrices can be compressed to save lots of memory.


## Sparse Matrices

The `Matrix` package extends the functionality of the basic `matrix` data type. Importantly, it allows use to create sparse matrices.

```{r}
    library(Matrix)
    A = Matrix(c(1,0,0,0,0,1,0,0,0,0), nrow=2, ncol=5, sparse=TRUE)
    A
```
Here the dots, represent the *sparse* values that will be compressed. The `Matrix` function has chosen to store our in a `dgCMatrix` object. This is the most general form of sparse matrix you will encounter, it makes use of the compressed sparse column format. There are also sparse matrices for more specific purposes such as `dgTMatrix` for triangular matrices and `dgSMatrix` for symmetric matrices. The dense format is known as `dgeMatrix`. The `Matrix` function often makes a good choice of choosing a good format for us, but we can select an appropriate matrix format we want by using R's built in `as()` function which performs *type coercion*. Typically, all of these matrices can be treated the same, as they all build on top of the base `matrix` type. The only noticeable difference should be the way the data is stored.

Here is an example showing how you can make a row-compressed sparse matrix by coercion from the base `matrix` type
```{r}
    A = as(matrix(c(1,0,0,0,0,1,0,0,0,0), nrow=2, ncol=5), "dgCMatrix")
```


Sparse matrices however are not always space efficient for small matrices. 
```{r}
    library(Matrix)
    A = Matrix(c(1,0,0,0,0,1,0,0,0,0), nrow=2, ncol=5, sparse=TRUE)
    B = matrix(c(1,0,0,0,0,1,0,0,0,0), nrow=2, ncol=5)
    # Compare object size
    c(object.size(A), object.size(B))
```
However, for any large matrices, the sparse implementations should be used.

## Solving Linear Systems and Matrix Inversion.

Solving systems of linear equations in R is easy. We can make use of the built-in `solve` function. If we have a system of equations $Ax = b$. We can simply find $x$.

```{r}
    A = matrix(c(1,2,3,4),nrow=2,ncol=2)
    b = c(1, 10)
    solve(A, b)
```

Alternatively, we could find the inverse of $A$ using `solve` and then use matrix multiplication.

```{r}
    invA = solve(A)
    invA

    invA %*% b
```

However, this tends to be slower and can often give less precise results. We cannot use `solve` in either case for singular matrices.

```{r, error=TRUE, results='asis'}
    A = matrix(c(1,2,2,4), nrow=2,ncol=2)
    solve(A, b)
```

## Matrix Operations

All of these functions work on both dense and sparse matrix implementations due to polymorphism.

### Eigenvectors and Eigenvalues
The eigenvectors and eigenvalues of a matrix are extremely easy to compute in R, this is done with the `eigen` function.
```{r}
    x = matrix(c(1,2,3,4), 2, 2)
    e = eigen(x)
    e$values
    e$vectors
```

### Single Value Decomposition
Similar to finding the eigenvectors and eigenvalues, the singular value decomposition of matrix can very easily be computed in R using the `svd` function.

```{r}
    x = matrix(1:4, 2, 2)
    x.svd = svd(x)
    x.svd$u
    x.svd$d
    x.svd$v
```

### Cholesky Factor

For a positive definite matrix $A$, we can find its Cholesky factor $R$ which is an upper triangular matrix that satisfies $R^TR = A$. For this reason it is often considered the square root of a matrix.

```{r}
    x = matrix(c(1,2,1,2), 2, 2)
    chol(x)
```

## LAPACK & BLAS

Most of the matrix operations we have seen so far in R are just wrappers to a highly efficient linear algebra library. Often, and in my case, this library is `LAPACK`. 

The linear algebra library that my system is using can be checked as follows
```{r}
    La_library()
```


`LAPACK` implements functions for solving systems of linear equations, computing eigenvectors & eigenvalues, computing the Cholesky decomposition and much more. `LAPACK` is written with performance in mind, it is designed to effectively make use of cpu-cache and is written in the compiled language `FORTRAN 90`.

`LAPACK` makes use of an even lower-level system library for basic linear algebra operations such as vector addition, scalar multiplication, dot products and matrix multiplication. `BLAS` (Basic Linear Algebra Subsystem) is a specification for these basic operations. There are many `BLAS` implementations which usually depends on which system you are running. Two popular implementations are `OpenBlas` and `MKL`, the first being an open-source implementation that is common to linux distributions and the second begin non-free software developed by intel. Having an efficient `BLAS` implementation is key for efficiency since these operations are performed incredibly often.

I can check the `BLAS` implementation I am running as follows
```{r}
    extSoftVersion()["BLAS"]
```
My system is using the reference implementation of `BLAS`, a stable but not a very highly optimized library - I could switch to Atlas/OpenBlas/MKL to try and get better performance!


\newpage

# Optimization

Here we will consider how to find approximate solutions to continuous optimization problems in R. These are precisely problems that are characterised by solving 
$$ \min_{\mathbf{x}} f(\mathbf{x}) $$
for an objective function $f$ where $\mathbf{x}$ is taken over some continuous domain.

Often we will have some constraints to our problem, typically of the form  $g(x) \le 0$ or $h(x) = 0$. These additional requirements gives us a **constrained** optimization problem which are generally harder to solve.

There exists many algorithms that attempt to find optimal solutions to these problems. Most of the common approaches are already implemented in R and we can take a look at them.

If instead, we want to find the maximum of a function $f$, we can just look for the minimum of $-f$. Following from this property, optimization in R is typically implemented to find the minimum of functions.

The algorithms we will look at all designed to final a *local minimum* of a objective function. If our objective functions is convex, then the local minimum is also the global minimum.

## Unconstrained Optimization

First of all, R comes with a built in `optimize` function that attempts to find the minimum of a function on single variable over a given range. We can specify a simple constaint, that is the interval our input can range over.

```{r}
    f = function(x) x^2 + 2 * x + 3
    optimize(f, interval=c(-5,5))
```
Although in this example it finds  the exact solution, the output of this function can be unpredictable for more complex functions so we should usually avoid it.

### Gradient Descent

Gradient descent methods attempt to find a local minimum by  moving along the curve in the opposite direction to the gradient at each point. Since the gradient gives the direction of the "fastest increase" moving in the opposite direction should gives us the fastest decrease. Gradient descent is typically performed by iterating the following
$$
    \mathbf{x}_n = \mathbf{x}_{n-1} - \gamma \nabla f(\mathbf{x}_{n-1})
$$

Here $\gamma$ is the step-size. This determines how much we should move in the opposite direction of gradient each step. For simplicity, we can choose it be constant. More effective gradient descent approaches use a systematic way of determining $\gamma$ on each iteration, such as line search.

```{r}
    f = function(x, y) {
        x^2 + 3*y^2 + 9 - x*y + 2*y
    }

    # Gradient
    G = function(x, y) {
        c(2 * x - y, 6 * y - x + 2)
    }

    gradient.descent = function(f, G, x, steps, gamma) {
        for (i in 1:steps) {
            x <- x - gamma * G(x[1], x[2])
        }
        return (x)
    }

    gradient.descent(f, G, c(1,0), 10000, gamma = 0.001)
```

The exact solution is $\left(-\frac{2}{11}, - \frac{4}{11}\right)$ so this is a fairly good estimate. However, this result largely depends on a good choice of parameters for the steps and gamma. Typically choosing a small gamma and a large steps will ensure better performance. A systematic way of choosing the step-size (gamma) can improve the accuracy and performance of this algorithm, for example line search.

### Stochastic Gradient Descent

Stochastic gradient descent is a gradient descent method that is commonly used to train neural networks. An ideal objective function of a neural network $Q$ would make use all of the data from a dataset $D$. However, this would make the gradient at each step very expensive to compute. Instead, if we  can break up the objective function into $n$ pieces
$$
Q(\mathbf{x}) = \frac{1}{n} \sum_{i=1}^nQ_i(\mathbf{x})
$$
where each $Q_i$ only involves using the i'th data point, we can use gradient descent for efficient training. Gradient descent can be performed by iterating the following
$$
    \mathbf{x}_{n} = \mathbf{x}_{n-1} - \gamma \nabla Q_i(\mathbf{x})
$$
In each iteration we are only considering a single data point, but by the end of the computation all of the data points have been involved in learning $x$. 

### Newton's Method

Newtons methods are based of the root finding algorithm
$$
x_n = x_{n - 1} - \frac{f(x_{n-1})}{f'(x_{n-1})}
$$
Applying this on the first derivative of our objective function, we can find the stationary points and therefore a minimum as long as we are being careful. For example, we can consider the sign of the second derivative. If it is positive (negative), we would be converging towards a minimum (maximum). If our objective function is convex we need not worry, this algorithm will always seek a minimum. Newton's algorithms are often faster than gradient descent methods, since the also use the Hessian.

A multivariate Newton method involves computing the gradient and the Hessian of the objective function. It can be performed by iterating the following
$$
\mathbf{x}_n = \mathbf{x}_{n - 1} - [\mathbf{H}f(x_{n-1})]^{-1}\nabla f(x_{n-1})
$$

We give a implementation of a basic multivariate Newton method below, where we have computed the Gradient and Hessian by hand. In practice, we wouldn't usually be able to do this.

```{r}
    f = function(x, y) {
        x^2 + 3*y^2 + 9 - x*y + 2*y
    }

    # Gradient
    G = function(x, y) {
        c(2 * x - y, 6 * y - x + 2)
    }

    # Hessian
    H = function(x, y) {
        matrix(c(2, 1, 1, 6), 2, 2)
    }

    newton.method = function(f, G, H, x, steps) {
        for (i in 1:steps) {
            x <- x - solve(H(x[1], x[2])) %*% G(x[1], x[2])
        }
        return (x)
    }
    newton.method(f, G, H, c(1,0), 10000)
```

Again, the exact solution is $\left(-\frac{2}{11}, - \frac{4}{11}\right)$ so this is a fairly good estimate.

Newton methods require both the gradient and the Hessian of the function we are minimizing. Although this provides good performance, finding these derivatives can be costly and sometimes infeasible. Furthermore, the Hessian also needs to be invertible. Typically we want the Hessian to be positive definite, since this guarantees that each iteration decreases the objective function. If this is not the case, we can perturbate the Hessian so that becomes positive definite.

Quasi-Newton methods are similar to Newton methods but do not require you to find the inverse Hessian. Instead they compute a reasonable estimate for it and this means that they are quicker.

## NLM & Optim

R includes two built-in functions that are actually useful for optimization and can be used with little concern. These are `nlm` and `optim`.

### NLM

The non-linear optimization function - `nlm` - uses a Newton style algorithm to find the minimum of a function. This of course involves computing the gradient and Hessian. We have two choices when using `nlm`, we give it an explicit form for the gradient and Hessian, or we can let it find a numerical estimate. We give examples that consider each approach.

Firstly, without an explicit gradient and Hessian

```{r}
    f = function(x) {
        x[1]^2 + 3*x[2]^2 + 9 - x[1]*x[2] + 2*x[2]
    }
    nlm(f, c(1,0)) 
```
Notice that `nlm` works on a function with a single arguments (that can be a vector).

Secondly, we can give it an explicit gradient and Hessian using the `deriv` function. The `deriv` function returns an `experssion` object. When evaluated, it returns the given objective functions value and has attributes giving the value of the gradient and Hessian at this point. `nlm`  recognises these attributes and makes use of them effectively.

```{r}
    f = deriv(expression( x^2 + 3*y^2 + 9 - x*y + 2*y), namevec = c('x', 'y')
              , function.arg=T, hessian=T)
    f(1,0)

    # Recall that nlm takes a function on a single vector input
    nlm(function(x) f(x[1], x[2]), c(1,0))
```

This gives us a good (and quick) estimate of the true global minimum!

### Optim

`optim` is another great built-in function for optimization. It select from a few common optimization algorithms including Nelder-Mead, quasi-Newton and conjugate-gradient. By default `optim` uses Nelder-Mead. Unlike `nlm` we cannot specify an explicit gradient and Hessian, instead `optim` estimates these numerically if needed depending the chosen algorithm. `optim` is as easy to use as `nlm`, although we specify its first two arguments in the reverse order.

We try out `optim` using a conjugate-gradient algorithm.
```{r}
    f = function(x) {
        x[1]^2 + 3*x[2]^2 + 9 - x[1]*x[2] + 2*x[2]
    }
    optim(c(1,0), f, method="CG")
```


\newpage


# Integration

## Quadrature Methods

Quadrature is mathematical term that means calculating area. So when we talk about "Quadrature Methods" we are just talking about methods for calculating area.

The most basic strategy we have for approximating the value of an integral relies on polynomial interpolation. This is just approximating a function using an integral. There are two key reason we want to use polynomials.

* Polynomials are very easy to integrate
* Any continuous function can approximated by polynomials to a given tolerance.

A simple but effective method of approximating a function by a polynomial is Lagrange interpolation. Given a sample of points $D = \{(x_i, f(x_i)\}_{i =1 } ^n$ that lie on the true curve $f$ we can construct a polynomial $p$ that approximates $f$ as follows
$$
    p(x) = \sum_{i = 1}^n \prod_{j = 1, j \neq i}^n \frac{x - x_j}{x_i - x_j}f(x_i)
$$
Then we can make the following approximation for the integral
$$
    \int_a^b f(x) dx \approx \int_a^bp(x) dx
$$
where the right hand side is easy to compute.

This type of polynomial interpolation does have an issue that we can address. Suppose that some region of the function $f$ behaves like an exponential function and another region acts like a sinusoidal function. A single polynomial couldn't effectively capture this behaviour. The parts of the polynomial that approximates the exponential region will still effect the region that behaves like a sinusoidal function. This is due to the global properties of a polynomial function. Therefore is often better to use many polynomials to interpolate small regions of the function $f$.

Suppose that we want to approximate 
$$
\int_a^b f(x) dx
$$

We can split the interval $[a, b]$ into $k$ many equally sized regions and in each of these intervals we approximate $f$ with polynomials $\{f_i\}_{i=1}^k$ (we can choose from many different methods to do this). Then we simply can estimate the integral

$$
     \int_a^b f(x) dx = \sum_{i = 1} ^{k } \int_{a + \frac{(i - 1)(b-a)}{k}}^{a +  \frac{i(b -a)}{k}}f_i(x)dx
$$
Again, the right hand side is easy to compute!

To use this method, we have to specify values for two variables:

* Polynomial degree
* Number of intervals

Typically the larger we choose these values to be, the better the approximation, although it will take longer to compute.

### The Integrate function

R's built in `integrate` function makes use of similar quadrature rules to perform numerical integration on a function. It can very easily be used as follows

```{r}
    f = function(x) sin(x) + x^2
    integrate(f, 0, 10)
```



## Monte-Carlo Integration

If we want to perform multivariate integration Monte-Carlo approaches are preferred. This is because they don't suffer from the curse of dimensionality that quadrature methods do.


### The Cubature Package

The `cubature` package can be used for Monte-Carlo integration which is specifically designed for multivariate integration. This package is just a wrapper to the `cubature` library that is implemented in C.

We will see how we can approximate the following multivariate integral using cubature
$$
    \int_0^1 \int_3^{10} \int_{-\pi}^{\pi}(sin(x) + y^2 + xy + yz)dxdydz
$$
```{r}
    library(cubature)
    f = function(x) {
        sin(x[1]) + x[2]^2 + x[1]* x[2] + x[2] * x[3]
    }
    cubintegrate(f, c(-1, 3, 0), c(1, 10, 1))
```

